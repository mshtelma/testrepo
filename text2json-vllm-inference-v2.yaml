name: text2json-vllm-inference

compute:
  cluster:
  gpus: 16

scheduling:
  resumable: true
  preemptible: true
  priority: lowest
  max_retries: 0

image: vllm/vllm-openai:v0.6.6.post1

env_variables:
  NCCL_DEBUG: "INFO"
  VLLM_SYNC_BACKEND: "nccl"
  NCCL_SOCKET_IFNAME: "eth0"
  GLOO_SOCKET_IFNAME: "eth0"

integrations:
  - integration_type: git_repo
    git_repo: mshtelma/testrepo
    git_branch: main



command: |
  pip install -U ray[default] openai pydantic
  huggingface-cli download deepseek-ai/DeepSeek-V3 --local-dir /model
  #huggingface-cli download meta-llama/Llama-3.1-8B-Instruct --local-dir /model
  
  if [ "${NODE_RANK}" == 0 ]; then
    echo "MASTER ${MASTER_ADDR}:6379"
    ray start  --head --port=6379
  else
    echo "SLAVE ${MASTER_ADDR}:6379"
    sleep 100
    ray start --block  --address="${MASTER_ADDR}:6379"
  fi
  sleep 400
  ray status
  
  if [ "${NODE_RANK}" == 0 ]; then
      vllm serve /model \
              --trust-remote-code \
              --tensor-parallel-size 8 \
              --pipeline-parallel-size 2 \
              --max-model-len 20000  \
              --served-model-name model \
              2>&1 | tee vllm_output.log &
      cd testrepo
      python3 waitformodel.py
      python3 testmodel.py
  fi

